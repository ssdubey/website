<p>IPFS (InterPlanetary file system) is a distributed file system built over peer to peer systems. It aims at creating a permanent web by involving peers to share data and make them available for other peers, instead of putting it at central location making it vulnerable to problems like Denial of service or Broken link issues.</p>

<p>How does it work
IPFS uses trackerless bittorrent based protocol to communicate among the participating nodes. It uses Kademlia DHT protocol with some modifications influenced by S/Kademlia and Coral DSHT.
Every participating node would maintain an IPFS directory structure describing the backend datastore and other configurations. 
IPFS offers 3 different datastores in the backend:
File system
Level DB
Badger DB</p>

<p>How to switch between datastores:
File system is the default datastore provided by IPFS. (how fs works)
Level DB and badger DB are other key-value data stores. (some thing about kv stores compared to fs)
Describe the steps to create level db and badger db</p>

<p>IPFS also offers an option to become a part of public IPFS network or create your own private network. A private IPFS network would identify the peer nodes based on a swarm key, which would be unique to that private network. If any public node shares that swarm key, then all the hosts would get exposed to the public network.</p>

<p>Major operations supported by IPFS would be:
Add: <code class="highlighter-rouge">ipfs add</code> command would accept a file, a directory or an input from stdin. Further it generates a hash corresponding to the data and adds it.</p>

<p>Cat: <code class="highlighter-rouge">ipfs cat</code> command takes a hash as input and prints the content of file or stdin on the screen.</p>

<p>Every object added to IPFS are considered as IPLD objects. <code class="highlighter-rouge">ipfs get</code> and <code class="highlighter-rouge">ipfs put</code> commands are used to manipulate IPLD objects.</p>

<p>We are working on integrating Irmin library datastore with IPFS to use its Routing system and content addressability capabilities to make Distributed applications based on Irmin more efficient. Since there are multiple storage options available with IPFS, we wanted to use the most optimal one in terms of load balancing, throughput, etc. This article is about our observation on benchmarking the IPFS on different parameters.</p>

<p>We have used all the three datastore backends provided by the IPFS for all the experiments. Also we attempted to compare the performance of IPFS working on a limited number of Private node collection and Public DHT. 
In this article we would mention the observations of the benchmark and also some of the interesting features of IPFS which we discovered on the way. 
Please feel free to comment and let us know if you want to know further details of the experiment or find any discrepancy in our understanding.</p>

<p>First parameter on our benchmark list is Size.
As mentioned before IPFS uses Bittorent like protocol for distributing data over the network. Typically bittorrent protocol divides the data into small blocks. By default the block size for IPFS is 256KB.
Our experiment makes the observation separately on data less than block size and more than that. Below are the graphical representation of our findings.</p>

<size related="" graphs="">
<img src="/assets/Sat_Jun_15_16:14:17_2019.png" alt="sdgsre" />

The above graphs show the performance of IPFS when the input size is less than the block size. It can be observed that if there is a large difference between the file size, we can observe some differences in processing time of `ipfs add` command. The “not so significant” difference in time taken for execution of command indicates the presence of an overhead component as part of the command execution. We will talk about this overhead in the later experiments.
</size>
